import re
import jieba
import os
from collections import Counter


# 1. æ–‡æœ¬æ¸…æ´—å‡½æ•°ï¼ˆè¿‡æ»¤HTMLæ ‡ç­¾ã€æ ‡ç‚¹ã€å†—ä½™æ–‡æœ¬ï¼‰
def clean_text(raw_text):
    # ç§»é™¤HTMLæ ‡ç­¾
    html_pattern = re.compile(r'<[^>]+>', re.S)
    text_no_html = html_pattern.sub('', raw_text)
    
    # ç§»é™¤â€œç½‘é¡µæ ‡é¢˜â€â€œç½‘é¡µé“¾æ¥â€ç­‰å†—ä½™å›ºå®šæ–‡æœ¬
    redundant_pattern = re.compile(r'ç½‘é¡µæ ‡é¢˜|ç½‘é¡µé“¾æ¥', re.S)
    text_no_redundant = redundant_pattern.sub('', text_no_html)
    
    # ä»…ä¿ç•™ä¸­æ–‡æ±‰å­—ï¼Œè¿‡æ»¤æ ‡ç‚¹/æ•°å­—/ç‰¹æ®Šç¬¦å·
    punctuation_pattern = re.compile(r'[^\u4e00-\u9fa5]', re.S)
    text_no_punct = punctuation_pattern.sub(' ', text_no_redundant)
    
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    text_clean = re.sub(r'\s+', ' ', text_no_punct).strip()
    return text_clean

# 2. è¯»å–æ–‡ä»¶å‡½æ•°
def read_file(filename):
    if not os.path.exists(filename):
        print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°æ–‡ä»¶ {filename}ï¼")
        return None
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            content = f.read()
        print(f"âœ… æˆåŠŸè¯»å– {filename}")
        return content
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥ï¼š{e}")
        return None

# 3. åˆ†è¯+è¯é¢‘ç»Ÿè®¡å‡½æ•°
def word_analysis(clean_text):
    # æ‰©å……åœç”¨è¯è¡¨ï¼ˆè¿‡æ»¤å¯¼èˆª/æ— æ„ä¹‰è¯æ±‡ï¼‰
    stop_words = {
        'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å’Œ', 'æœ‰', 'å°±', 'éƒ½', 'è¿™', 'é‚£', 'ä¸ª', 'ä¸º', 'æŠŠ',
        'é¦–é¡µ', 'é“¾æ¥', 'ç½‘é¡µ', 'ç™»å½•', 'é€€å‡º', 'æŠ¥å', 'ç”³è¯·', 'é€šçŸ¥å…¬å‘Š', 'ç«èµ›é€šçŸ¥',
        'æ–°é—»é¦–é¡µ', 'å¤§èµ›æ–°é—»', 'å­¦ä¼šæ–°é—»', 'ç²¾å½©å›é¡¾', 'ç«èµ›ä½œå“', 'æ ¡èµ›ç”³è¯·', 'å…³äº', 'å¹´', 'å±Š'
    }
    
    # jiebaç²¾ç¡®åˆ†è¯
    word_list = jieba.lcut(clean_text)
    # è¿‡æ»¤åœç”¨è¯+å•å­—
    filtered_words = [w for w in word_list if w not in stop_words and len(w) > 1]
    
    # ç»Ÿè®¡è¯é¢‘ï¼Œå–TOP20
    word_freq = Counter(filtered_words)
    top20_words = word_freq.most_common(20)
    
    return filtered_words, top20_words  # è¿”å›ï¼šåˆ†è¯ç»“æœã€TOP20è¯é¢‘

# 4. ä¿å­˜ç»“æœï¼ˆåˆ†è¯ç»“æœæ¢è¡Œæ˜¾ç¤ºï¼Œä»…ä¿ç•™TOP20+åˆ†è¯ç»“æœï¼‰
def save_results(filtered_words, top20_words):
    with open('words.txt', 'w', encoding='utf-8') as f:
        # ç¬¬ä¸€éƒ¨åˆ†ï¼šå®Œæ•´åˆ†è¯ç»“æœï¼ˆæ¯è¡Œæ˜¾ç¤º10ä¸ªè¯ï¼Œæ¢è¡Œï¼‰
        f.write("===== æ–‡æœ¬åˆ†è¯ç»“æœ =====\n")
        # æ¯10ä¸ªåˆ†è¯æ¢ä¸€è¡Œï¼Œæå‡å¯è¯»æ€§
        line_words = []
        for idx, word in enumerate(filtered_words, 1):
            line_words.append(word)
            # æ¯10ä¸ªè¯æ¢è¡Œï¼Œæˆ–æœ€åä¸è¶³10ä¸ªè¯æ—¶æ¢è¡Œ
            if idx % 10 == 0 or idx == len(filtered_words):
                f.write(' '.join(line_words) + "\n")
                line_words = []
        
        # ç©ºè¡Œåˆ†éš”ï¼Œæå‡æ ¼å¼æ•´æ´åº¦
        f.write("\n")
        
        # ç¬¬äºŒéƒ¨åˆ†ï¼šTOP20é«˜é¢‘è¯
        f.write("===== è¯é¢‘æœ€é«˜çš„20ä¸ªè¯ =====\n")
        for idx, (word, count) in enumerate(top20_words, 1):
            f.write(f"{idx:2d}. {word:<8} å‡ºç°æ¬¡æ•°ï¼š{count}\n")
    
    print("\nâœ… words.txtå·²ç”Ÿæˆï¼ˆåˆ†è¯ç»“æœæ¢è¡Œæ˜¾ç¤ºï¼‰")
    # æ§åˆ¶å°è¾“å‡ºTOP20
    print("\n===== TOP20 é«˜é¢‘è¯ =====")
    for idx, (word, count) in enumerate(top20_words, 1):
        print(f"{idx:2d}. {word:<8} å‡ºç°æ¬¡æ•°ï¼š{count}")

# ä¸»å‡½æ•°
def main():
    target_file = "new1.txt"
    # è¯»å–æ–‡ä»¶
    raw_content = read_file(target_file)
    if not raw_content:
        return
    
    # æ¸…æ´—æ–‡æœ¬
    clean_content = clean_text(raw_content)
    
    # åˆ†è¯+ç»Ÿè®¡TOP20
    filtered_words, top20_words = word_analysis(clean_content)
    
    # ä¿å­˜ç»“æœï¼ˆåˆ†è¯æ¢è¡Œ+TOP20ï¼‰
    save_results(filtered_words, top20_words)
    print("\nğŸ‰ å¤„ç†å®Œæˆï¼")

if __name__ == '__main__':
    jieba.initialize()
    main()
